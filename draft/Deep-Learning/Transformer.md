---
title: "[Deep Learning] 트렌스포머"
category:
    - "Deep Learning"
tags:
    - "NLP", "Transformer"
toc: true
use_math: true
---
참고문헌

https://bkshin.tistory.com/entry/NLP-14-%EC%96%B4%ED%85%90%EC%85%98Attention#:~:text=%ED%95%9C%EA%B5%AD%EC%96%B4%EB%A5%BC%20%EC%98%81%EC%96%B4%EB%A1%9C%20%EB%B2%88%EC%97%AD,%EB%AC%B8%EC%9E%A5%20%EB%B3%80%ED%99%98%ED%95%98%EB%8A%94%20%EA%B2%83%EC%9E%85%EB%8B%88%EB%8B%A4.

https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=Transformers%20use%20a%20smart%20positional,summed%20with%20its%20positional%20information.

https://github.com/bentrevett/pytorch-seq2seq
